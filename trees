import nltk
nltk.download('punkt')
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer

import json
import pickle

import numpy as np
from numpy.random import RandomState
import tensorflow as tf
from tensorflow import keras
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Activation, Dropout
#from tensorflow.python.keras.optimizers import SGD
from tensorflow.keras.optimizers import SGD
from sklearn import tree

import random
import pandas as pd

def trees1():
    utterances = []
    dialog_act = []
    with open(r'raw_files/dialog_acts.dat', "r") as file:
        for line in file:
            parts = line.strip().split(maxsplit=1)  
            if len(parts) == 2:
                dialog_act.append(parts[0].strip().lower())  
                utterances.append(parts[1].strip().lower())  

    #print(dialog_act)
    
    lemmatizer = WordNetLemmatizer()
    words=[]
    classes = []
    documents = []
    ignore_words = ['?', '!']
    #data_file = open('D:\FlaskChatbot\intents.json').read()
    #intents = json.loads(data_file)
    i = 0 
    while i < len(dialog_act):
        i = i + 1
        for word in dialog_act[i-1]:
            w = nltk.word_tokenize(word)
            words.extend(w)
            documents.append((w, dialog_act['tag']))
            if dialog['tag'] not in classes:
                classes.append(dialog['tag'])
    words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
    words = sorted(list(set(words)))

    classes = sorted(list(set(classes)))
    #print (len(documents), "word with act")
    #print (len(classes), "act", classes)
    #print (len(words), "unique lemmatized words", words)
    pickle.dump(words,open('words.pkl','wb'))
    pickle.dump(classes,open('classes.pkl','wb'))
    
    training = []
    output_empty = [0] * len(classes)
    
    for doc in documents:
        bag = []
        pattern_words = doc[0]
        pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
        for w in words:
            bag.append(1) if w in pattern_words else bag.append(0)
        output_row = list(output_empty)
        output_row[classes.index(doc[1])] = 1
        training.append([bag, output_row])
    random.shuffle(training)
    training = np.array(training,dtype=object)
    train_x = list(training[:,0])
    train_y = list(training[:,1])
    print("Training data created")

    X = train_x #utterence
    Y = train_y #dialog acts 
    clf = tree.DecisionTreeClassifier()
    clf = clf.fit(X, Y)
    tree.plot_tree(clf)
    #return(clf.predict("user input here"))

def trees2():
    nltk.download('punkt')
    nltk.download('wordnet')
    lemmatizer = WordNetLemmatizer()
    words=[]
    classes = []
    documents = []
    ignore_words = ['?', '!']
    data_file = open('D:\FlaskChatbot\intents.json').read()
    intents = json.loads(data_file)
    for intent in intents['intents']:
        for pattern in intent['patterns']:
            w = nltk.word_tokenize(pattern)
            words.extend(w)
            documents.append((w, intent['tag']))
            if intent['tag'] not in classes:
                classes.append(intent['tag'])
    words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
    words = sorted(list(set(words)))

    classes = sorted(list(set(classes)))
    print (len(documents), "documents")
    print (len(classes), "classes", classes)
    print (len(words), "unique lemmatized words", words)
    pickle.dump(words,open('words.pkl','wb'))
    pickle.dump(classes,open('classes.pkl','wb'))
    training = []
    output_empty = [0] * len(classes)
    
    for doc in documents:
        bag = []
        pattern_words = doc[0]
        pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
        for w in words:
            bag.append(1) if w in pattern_words else bag.append(0)
        output_row = list(output_empty)
        output_row[classes.index(doc[1])] = 1
        training.append([bag, output_row])
    random.shuffle(training)
    training = np.array(training,dtype=object)
    train_x = list(training[:,0])
    train_y = list(training[:,1])
    print("Training data created")

    X = train_x #utterence
    Y = train_y #dialog acts 
    clf = tree.DecisionTreeClassifier()
    clf = clf.fit(X, Y)
    tree.plot_tree(clf)
    #return(clf.predict("user input here"))

if __name__ == "__main__":
    #majority_class_bot()
    #rule_based_chat_bot()
    #data_prep()
    #trees1()
    trees2()